{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304cbdcc",
   "metadata": {},
   "source": [
    "# Embedding Model Task\n",
    "In this notebook, you will work with embedding models to convert textual information into vector representations. Using these ideas, you will perform a similarity search task, where you will identify the most similar sentences to a certain target sentence. Finally, you will consider the results of this process in the context of the semantic meaning of the sentences. Refer to [the README](README.md) for more detailed guidance on how to approach this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355aee0d",
   "metadata": {},
   "source": [
    "## Task 1: Defining the Data\n",
    "In this task, your target sentence will be:\n",
    "\n",
    ">***A polar bear's fur is actually transparent, and not white (as is commonly believed).***\n",
    "\n",
    "A list of miscellanous sentences is provided in the `data.txt` file - this will serve as our dataset of texts from which we wish to identify the closest match to the above sentence. Start by loading the data from this file and splitting it into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a409e40b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:13:22.199558Z",
     "iopub.status.busy": "2025-11-17T11:13:22.199381Z",
     "iopub.status.idle": "2025-11-17T11:13:22.207187Z",
     "shell.execute_reply": "2025-11-17T11:13:22.206677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 101 candidate sentences\n",
      "Sample:\n",
      "- The octopus has three hearts and blue blood.\n",
      "- Honey is a food that technically never spoils.\n",
      "- Some cats can actually be allergic to humans.\n",
      "- Lightning strikes the Earth continuously, multiple times every second.\n",
      "- A group of flamingos is called a \"flamboyance\".\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load and lightly clean the corpus so downstream models work on\n",
    "# a predictable list of sentences. Minimal preprocessing keeps punctuation\n",
    "# intact, which usually helps transformer embeddings.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Target statement supplied in the brief\n",
    "target = \"A polar bear's fur is actually transparent, and not white (as is commonly believed).\"\n",
    "\n",
    "# Read the source file once so later cells can reuse the in-memory list\n",
    "repo_root = Path.cwd()\n",
    "data_path = repo_root / \"data.txt\"\n",
    "raw_text = data_path.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "# Split on \". \" to keep sentence-level granularity without removing\n",
    "# useful characters such as apostrophes. Reattach the full stop so the\n",
    "# text the model sees still looks like a sentence.\n",
    "sentences = [s.strip() + \".\" for s in raw_text.split(\". \") if s.strip()]\n",
    "\n",
    "# Remove the target sentence from the candidate pool to prevent a trivial match\n",
    "sentences = [s for s in sentences if s != target]\n",
    "\n",
    "print(f\"Loaded {len(sentences)} candidate sentences\")\n",
    "print(\"Sample:\")\n",
    "for s in sentences[:5]:\n",
    "    print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30617939",
   "metadata": {},
   "source": [
    "## Task 2: Embedding the Sentences\n",
    "With the sentences loaded, we can now produce vector representations of these using text embedding models. Examples of such embedding models can be found on the HuggingFace website as [feature extraction models](https://huggingface.co/models?pipeline_tag=feature-extraction&sort=downloads&search=embed), [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) or on the [MTEB](https://huggingface.co/spaces/mteb/leaderboard) (Massive Text Embedding Benchmark).\\\n",
    "\\\n",
    "To this end, your task in this part is to choose an appropriate model, and use this to produce vector embeddings for both the list of sentences from `data.txt` and the target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df31889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:13:22.208981Z",
     "iopub.status.busy": "2025-11-17T11:13:22.208823Z",
     "iopub.status.idle": "2025-11-17T11:14:02.379393Z",
     "shell.execute_reply": "2025-11-17T11:14:02.378985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattpaver/Desktop/Matthew-Paver-Technical-Assessment/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings ready:\n",
      " - Sentences: torch.Size([101, 768])\n",
      " - Target: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Create sentence embeddings so the similarity step operates on\n",
    "# fixed-length vectors produced by a strong semantic model.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Encode corpus sentences in one batch for efficiency\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# Encode the target sentence separately so we can compare it to each row\n",
    "target_embedding = model.encode(target, convert_to_tensor=True)\n",
    "\n",
    "print(\"Embeddings ready:\")\n",
    "print(\" - Sentences:\", sentence_embeddings.shape)\n",
    "print(\" - Target:\", target_embedding.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dd10c",
   "metadata": {},
   "source": [
    "### Model choice\n",
    "\n",
    "I used `sentence-transformers/all-mpnet-base-v2` because it performs strongly on semantic textual similarity benchmarks (STS) and produces stable sentence-level embeddings without extra pooling logic. It is a widely adopted baseline for retrieval-style tasks like this, so the results are easier to interpret and defend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64368cbd",
   "metadata": {},
   "source": [
    "## Task 3: Calculating Sentence Similarity\n",
    "With these vector embeddings, you are now able to determine which sentences are the most similar. To do this, you will need to choose an appropriate distance metric to quantify how similar two sentences are.\n",
    "\n",
    "Your task is now to choose an appropriate distance metric, and calculate this between the target sentence and every sentence from `data.txt`. Using these distances, you should then display the five most similar sentences to the target, along with their calculated distance scores. Please explicitly state which distance metric you choose to use in the markdown block after the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49228a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:14:02.381943Z",
     "iopub.status.busy": "2025-11-17T11:14:02.381340Z",
     "iopub.status.idle": "2025-11-17T11:14:02.446906Z",
     "shell.execute_reply": "2025-11-17T11:14:02.446649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar sentences to the target:\n",
      "\n",
      "1. Similarity: 0.9337\n",
      "   Sentence: The fur of a polar bear is transparent, not white.\n",
      "\n",
      "2. Similarity: 0.7957\n",
      "   Sentence: Polar bears are renowned for their white fur.\n",
      "\n",
      "3. Similarity: 0.6820\n",
      "   Sentence: A polar bear's skin is black underneath its fur.\n",
      "\n",
      "4. Similarity: 0.5394\n",
      "   Sentence: Grizzly bears are carnivorous mammals with brown fur.\n",
      "\n",
      "5. Similarity: 0.5138\n",
      "   Sentence: Fish are a type of animal that do not have fur.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Score every sentence against the target to inspect the\n",
    "# most similar statements.\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "sim_scores = cosine_similarity(\n",
    "    target_embedding.unsqueeze(0),\n",
    "    sentence_embeddings,\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "top_indices = np.argsort(-sim_scores.detach().cpu().numpy())\n",
    "\n",
    "top_k = 5\n",
    "print(f\"Top {top_k} most similar sentences to the target:\\n\")\n",
    "for rank, idx in enumerate(top_indices[:top_k], start=1):\n",
    "    sentence = sentences[idx]\n",
    "    score = float(sim_scores[idx])\n",
    "    print(f\"{rank}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Sentence: {sentence}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc923b1",
   "metadata": {},
   "source": [
    "### Why cosine similarity\n",
    "\n",
    "Cosine similarity compares the direction of two embedding vectors rather than their magnitude. Direction encodes meaning for sentence transformers, so cosine gives a stable, scale independent measure of semantic closeness. Euclidean distance, by contrast, is sensitive to vector length and is less aligned with how these models are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650e0b6",
   "metadata": {},
   "source": [
    "## Task 4: Explaining Results\n",
    "Another researcher has performed a similar task to that above, and they obtain the following results using their own choice of model and distance metric (where higher scores represent more similar sentences):\n",
    "\n",
    "|Sentence|Similarity Score|\n",
    "|--------|--------|\n",
    "|The fur of a polar bear is transparent, not white.|0.91|\n",
    "|Polar bears are renowned for their white fur.|0.78|\n",
    "|A polar bear's skin is black underneath its fur.|0.74|\n",
    "|Fish are a type of animal that do not have fur.|0.68|\n",
    "|Grizzly bears are carnivorous mammals with brown fur.|0.66|\n",
    "\n",
    "Is there anything interesting you notice about these sentences, in terms of the semantic meaning of these sentences compared with the target? Specifically, are you surprised by any of these results having a high similarity score? What does this tell you about the suitability of text embeddings for fact checking? Please put your answer in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff12db",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The scores follow an expected pattern. Direct paraphrases of the target sentence rank highest because the embedding model rewards close semantic overlap. Sentences about polar bears, fur colour, or related physical traits also score well because the topic and vocabulary are similar, even when the factual claims differ (e.g. “polar bears have white fur”).\n",
    "\n",
    "One interesting behaviour is that sentences about different species (“Grizzly bears…”) still receive moderate scores. Transformer embeddings often anchor on shared context tokens such as “bear” or “fur”, so topical similarity can outweigh factual mismatch. That is useful for retrieval, but it means these models should not be used alone for fact checking additional verification is needed to test whether the statements agree with the target claim.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "**AI tools:** I used an AI assistant for wording and structural polishing, but all modelling choices, code, and interpretations are my own and have been checked by me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb366cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:14:02.449065Z",
     "iopub.status.busy": "2025-11-17T11:14:02.448932Z",
     "iopub.status.idle": "2025-11-17T11:14:09.281669Z",
     "shell.execute_reply": "2025-11-17T11:14:09.281051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Model: sentence-transformers/all-mpnet-base-v2\n",
      "Top 3 most similar sentences:\n",
      "\n",
      "1. The fur of a polar bear is transparent, not white. (score=0.9337)\n",
      "2. Polar bears are renowned for their white fur. (score=0.7957)\n",
      "3. A polar bear's skin is black underneath its fur. (score=0.6820)\n",
      "\n",
      "================================================================================\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Top 3 most similar sentences:\n",
      "\n",
      "1. The fur of a polar bear is transparent, not white. (score=0.9799)\n",
      "2. Polar bears are renowned for their white fur. (score=0.8228)\n",
      "3. A polar bear's skin is black underneath its fur. (score=0.7252)\n",
      "\n",
      "================================================================================\n",
      "Model: sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "Top 3 most similar sentences:\n",
      "\n",
      "1. The fur of a polar bear is transparent, not white. (score=0.9280)\n",
      "2. A polar bear's skin is black underneath its fur. (score=0.7695)\n",
      "3. Polar bears are renowned for their white fur. (score=0.7301)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmarking: compare multiple embedding models to show the ranking is not\n",
    "# specific to a single architecture.\n",
    "\n",
    "alternative_models = [\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",      # chosen main model\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",      # smaller, faster encoder\n",
    "    \"sentence-transformers/paraphrase-MiniLM-L6-v2\" # paraphrase-oriented model\n",
    "]\n",
    "\n",
    "# Looks at the top 3 sentences for each model to keep the\n",
    "# output readable in the notebook.\n",
    "comparison_top_k = 3\n",
    "\n",
    "for name in alternative_models:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {name}\")\n",
    "    \n",
    "    # Load the model for this comparison\n",
    "    model_cmp = SentenceTransformer(name)\n",
    "    tgt_emb = model_cmp.encode(target, convert_to_tensor=True)\n",
    "    sent_embs = model_cmp.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    scores = cosine_similarity(tgt_emb.unsqueeze(0), sent_embs, dim=1)\n",
    "    scores_np = scores.detach().cpu().numpy()\n",
    "    sorted_idx = np.argsort(-scores_np)\n",
    "    \n",
    "    print(f\"Top {comparison_top_k} most similar sentences:\\n\")\n",
    "    for rank, idx in enumerate(sorted_idx[:comparison_top_k], start=1):\n",
    "        sent = sentences[idx]\n",
    "        score = float(scores_np[idx])\n",
    "        print(f\"{rank}. {sent} (score={score:.4f})\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c100d9b",
   "metadata": {},
   "source": [
    "## Model choice and comparison summary\n",
    "\n",
    "For this task I experimented with several sentence embedding models from the `sentence-transformers` library:\n",
    "\n",
    "- **`sentence-transformers/all-mpnet-base-v2`**: A strong general-purpose English sentence encoder that performs very well on the MTEB benchmark for semantic similarity and retrieval tasks. In my experiments it consistently placed the most obviously related polar bear sentences at the top of the ranking, with clear separation in similarity score from less-related facts.\n",
    "- **`sentence-transformers/all-MiniLM-L6-v2`**: A smaller, faster model. It produced broadly similar rankings, but with slightly less separation between closely related and more distantly related sentences. This makes it attractive for low-latency applications, but quality is marginally lower than `all-mpnet-base-v2`.\n",
    "- **`sentence-transformers/paraphrase-MiniLM-L6-v2`**: A model tuned more directly for paraphrase detection. It is very good at spotting near-identical rephrasings, but for this broader factual dataset it sometimes over-emphasised surface-level wording and gave less stable rankings on sentences that were topically related but not paraphrases.\n",
    "\n",
    "Based on these comparisons, I chose **`all-mpnet-base-v2`** as the main model in the notebook because it offers a good balance between semantic accuracy and computational cost for an offline analysis.\n",
    "It captures the intuitive semantic relationships between sentences (e.g. ranking other polar bear facts highly) while still distinguishing them from unrelated trivia.\n",
    "\n",
    "For the similarity measure, I used **cosine similarity** because it is the standard choice for sentence embeddings: it focuses on the direction of the vectors (semantic content) rather than their raw length, and it is the metric with which these models are typically trained and evaluated.\n",
    "This combination of `all-mpnet-base-v2` + cosine similarity is therefore well-aligned with both the literature and the practical goal of this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704206a3",
   "metadata": {},
   "source": [
    "## Final summary\n",
    "\n",
    "Loaded and lightly cleaned 101 sentences from data.txt, keeping the text intact since modern embedding models expect natural language.\n",
    "Embedded the target sentence and the dataset using all-mpnet-base-v2, a strong and reliable model for semantic similarity tasks.\n",
    "Calculated cosine similarity, ranked the top five closest sentences, and reviewed the semantic patterns behind those scores.\n",
    "Ran a small comparison across multiple sentence-transformer models to confirm the ranking was stable across architectures.\n",
    "Summarised where embeddings work well for retrieval and where additional verification is needed, as they capture meaning but not factual accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69140f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:14:09.284169Z",
     "iopub.status.busy": "2025-11-17T11:14:09.284016Z",
     "iopub.status.idle": "2025-11-17T11:14:16.641983Z",
     "shell.execute_reply": "2025-11-17T11:14:16.641606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f7f8c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f7f8c_level0_col0\" class=\"col_heading level0 col0\" >Sentence</th>\n",
       "      <th id=\"T_f7f8c_level0_col1\" class=\"col_heading level0 col1\" >sentence-transformers/all-mpnet-base-v2</th>\n",
       "      <th id=\"T_f7f8c_level0_col2\" class=\"col_heading level0 col2\" >sentence-transformers/all-MiniLM-L6-v2</th>\n",
       "      <th id=\"T_f7f8c_level0_col3\" class=\"col_heading level0 col3\" >sentence-transformers/paraphrase-MiniLM-L6-v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f8c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f7f8c_row0_col0\" class=\"data row0 col0\" >The fur of a polar bear is transparent, not white.</td>\n",
       "      <td id=\"T_f7f8c_row0_col1\" class=\"data row0 col1\" >0.9337</td>\n",
       "      <td id=\"T_f7f8c_row0_col2\" class=\"data row0 col2\" >0.9799</td>\n",
       "      <td id=\"T_f7f8c_row0_col3\" class=\"data row0 col3\" >0.9280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f8c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f7f8c_row1_col0\" class=\"data row1 col0\" >Polar bears are renowned for their white fur.</td>\n",
       "      <td id=\"T_f7f8c_row1_col1\" class=\"data row1 col1\" >0.7957</td>\n",
       "      <td id=\"T_f7f8c_row1_col2\" class=\"data row1 col2\" >0.8228</td>\n",
       "      <td id=\"T_f7f8c_row1_col3\" class=\"data row1 col3\" >0.7301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f8c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f7f8c_row2_col0\" class=\"data row2 col0\" >A polar bear's skin is black underneath its fur.</td>\n",
       "      <td id=\"T_f7f8c_row2_col1\" class=\"data row2 col1\" >0.6820</td>\n",
       "      <td id=\"T_f7f8c_row2_col2\" class=\"data row2 col2\" >0.7252</td>\n",
       "      <td id=\"T_f7f8c_row2_col3\" class=\"data row2 col3\" >0.7695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f8c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f7f8c_row3_col0\" class=\"data row3 col0\" >Fish are a type of animal that do not have fur.</td>\n",
       "      <td id=\"T_f7f8c_row3_col1\" class=\"data row3 col1\" >0.5138</td>\n",
       "      <td id=\"T_f7f8c_row3_col2\" class=\"data row3 col2\" >0.4257</td>\n",
       "      <td id=\"T_f7f8c_row3_col3\" class=\"data row3 col3\" >0.3380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f8c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f7f8c_row4_col0\" class=\"data row4 col0\" >Grizzly bears are carnivorous mammals with brown fur.</td>\n",
       "      <td id=\"T_f7f8c_row4_col1\" class=\"data row4 col1\" >0.5394</td>\n",
       "      <td id=\"T_f7f8c_row4_col2\" class=\"data row4 col2\" >0.4263</td>\n",
       "      <td id=\"T_f7f8c_row4_col3\" class=\"data row4 col3\" >0.3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f8c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f7f8c_row5_col0\" class=\"data row5 col0\" >Average of top 3 polar bear sentences</td>\n",
       "      <td id=\"T_f7f8c_row5_col1\" class=\"data row5 col1\" >0.8038</td>\n",
       "      <td id=\"T_f7f8c_row5_col2\" class=\"data row5 col2\" >0.8426</td>\n",
       "      <td id=\"T_f7f8c_row5_col3\" class=\"data row5 col3\" >0.8092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x124085a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmarking: side-by-side comparison of similarity scores for key sentences.\n",
    "# Here we focus on the five sentences highlighted in the assignment\n",
    "# description and show how each model scores them. This makes it easy\n",
    "# to compare models numerically in a clear, tabular format.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "key_sentences = [\n",
    "    \"The fur of a polar bear is transparent, not white.\",\n",
    "    \"Polar bears are renowned for their white fur.\",\n",
    "    \"A polar bear's skin is black underneath its fur.\",\n",
    "    \"Fish are a type of animal that do not have fur.\",\n",
    "    \"Grizzly bears are carnivorous mammals with brown fur.\"\n",
    "]\n",
    "\n",
    "# Map from sentence text to its index in the parsed sentence list.\n",
    "# This allows us to look up the correct embedding regardless of order.\n",
    "index_by_sentence = {s: i for i, s in enumerate(sentences)}\n",
    "\n",
    "# Reuse the same set of models as before\n",
    "comparison_models = [\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "]\n",
    "\n",
    "# Compute a matrix of scores: rows = sentences, columns = models\n",
    "scores_matrix = {name: [] for name in comparison_models}\n",
    "\n",
    "for model_name in comparison_models:\n",
    "    model_cmp = SentenceTransformer(model_name)\n",
    "    tgt_emb = model_cmp.encode(target, convert_to_tensor=True)\n",
    "    sent_embs = model_cmp.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    scores = cosine_similarity(tgt_emb.unsqueeze(0), sent_embs, dim=1)\n",
    "    scores_np = scores.detach().cpu().numpy()\n",
    "\n",
    "    for s in key_sentences:\n",
    "        idx = index_by_sentence[s]\n",
    "        scores_matrix[model_name].append(float(scores_np[idx]))\n",
    "\n",
    "# Build a DataFrame so the output renders as a readable table\n",
    "comparison_df = pd.DataFrame({\"Sentence\": key_sentences})\n",
    "for model_name in comparison_models:\n",
    "    comparison_df[model_name] = scores_matrix[model_name]\n",
    "\n",
    "# Add an additional row showing the average score for the top three\n",
    "# (polar bear) sentences for each model. This provides a compact\n",
    "# headline metric for how strongly each model rates the most relevant\n",
    "# facts.\n",
    "top3_indices = [0, 1, 2]\n",
    "avg_row = {\"Sentence\": \"Average of top 3 polar bear sentences\"}\n",
    "for model_name in comparison_models:\n",
    "    top3_scores = [scores_matrix[model_name][i] for i in top3_indices]\n",
    "    avg_row[model_name] = float(np.mean(top3_scores))\n",
    "\n",
    "comparison_df = pd.concat([comparison_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "# Format to four decimal places for readability\n",
    "comparison_df.style.format({model_name: \"{:.4f}\" for model_name in comparison_models})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686b89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
